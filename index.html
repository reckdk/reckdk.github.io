
<!-- saved from url=(0041)http://people.eecs.berkeley.edu/~junyanz/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<script async src="./analytics.js"></script><script type="text/javascript" src="./hidebib.js"></script>
<title>Jiaxiang Ren's Homepage</title>
<link rel="shortcut icon" href="http://cg.cs.tsinghua.edu.cn/people/~xianying/favicon.ico">
<style type="text/css">
body {
	margin-top: 30px;
	margin-bottom: 30px;
	margin-left: 100px;
	margin-right: 100px;
}
p {
	margin-top: 0px;
	margin-bottom: 0px;
}

.caption {
	font-size: 34px;
	font-weight: normal;
	color: #000;
	font-family: Constantia, "Lucida Bright", "DejaVu Serif", Georgia, serif;
}
.caption-1 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
}
.caption-2 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	font-weight: bold;
	color: #990000;
}
.caption-3 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	font-weight: bold;
	color: #F00;
}
.caption-4 {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #990000;
}
.content {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	text-align: justify;
}
.content a {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #000;
}
.content strong a {
	font-size: 16px;
	font-family: Tahoma, Geneva, sans-serif;
	color: #990000;
}
heading {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
  font-size: 15px;
  font-weight: 700
}
.title-small {
	font-size: 20px;
	font-family: Georgia, "Times New Roman", Times, serif;
	font-weight: bold;
	color: #F90;
}
.title-large {
	font-size: 28px;
	font-family: Georgia, "Times New Roman", Times, serif;
	font-weight: bold;
	color: #000;
}
.margin {
	font-size: 10px;
	line-height: 10px;
}
.margin-small {
	font-size: 5px;
	line-height: 5px;
}
.margin-large {
	font-size: 16px;
	line-height: 16px;
}
a:link {
	text-decoration: none;
}
a:visited {
	text-decoration: none;
}
content a:link {
	text-decoration: none;
}
content a:visited {
	text-decoration: none;
}
a:hover {
	text-decoration: underline;
}
a:active {
	text-decoration: underline;
	color: #93291b;
	font-family: Tahoma, Geneva, sans-serif;
}
strong a:active {
	text-decoration: underline;
	color: #93291b;
}
</style>
<script async="" src="./files/analytics.js.download"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script></head>



<body>

<table border="0" width="100%">
  <tbody>

    <tr>
    <td width="185"><img src="./files/Reck_2020SBU.jpeg" border="1" height="250"></td>
    <td width="15"></td>
    <td></td>
    <td><table border="0" width="100%">
      <tbody><tr height="10">
        <td colspan="2"></td></tr>


         <tr height="20">
        <td>
           <p class="caption">Jiaxiang Ren</p>
           <!--<p class="content">Algorithm Engineer</p>
           <p class="content">Ping An Technology</p>
           <p class="content">Shanghai, China</p>
				 -->
        </td>
      </tr>

      <tr height="40">
        <td><table border="0" width="100%">
          <tbody><tr height="20">
            <td width="55">
              <p class="content"><strong>Email: </strong></p></td>
            <td>
              <p class="content"> jiaxren AT cs.stonybrook.edu</p></td>
          </tr>
        </tbody></table></td>
      </tr>

      <tr height="20">
        <td>
          <p class="margin">&nbsp;</p>
          <p class="content">
            <strong><a href="./files/CV_Jiaxiang_Ren.pdf">Curriculum Vitae</a></strong> |
          <strong><a href="https://github.com/reckdk/">GitHub</a></strong> </p>
        </td>
      </tr>
      <tr height="20">
        <td colspan="2"></td></tr>
    </tbody></table></td>
  </tr>
</tbody></table>
<p class="margin">&nbsp;</p>

<table border="0">
  <tbody>
    <tr>
      <td width="1000"> <p align="justify" class="content">I am a CS PhD student at the Department of Computer Science of <strong><a href="https://www.cs.stonybrook.edu" target="_blank" rel="nofollow" class="caption-2" style="color: #000">Stony Brook University</a></strong>, under the supervision of <strong><a href="https://www3.cs.stonybrook.edu/~hling/" target="_blank" rel="nofollow" class="caption-2" style="color: #000">Prof. Haibin Ling</a></strong> and <strong><a href="https://www.stonybrook.edu/commcms/bme/people/y_pan.php" target="_blank" rel="nofollow" class="caption-2" style="color: #000">Prof. Yingtian Pan</a></strong>. My research interests mainly focus on Computer Vision and pattern recognition.<br \>
			Previously, I was an algorithm engineer in <a href="https://tech.pingan.com/en/">Ping An Technology</a>, working on computer vision, machine learning and deep learning.
      I obtained the BEng and MS degree in Computer Science and Technology from <a href="https://en.tongji.edu.cn/index.htm">Tongji University</a>, advised by <a href="http://sse.tongji.edu.cn.https.jxutcmtsg.proxy.jxutcm.edu.cn/info/1092/3140.htm"> Prof. Shengjie Zhao</a>.<br></br>
    </td></tr>
  </tbody>
</table>


<!--
<br>

<p id="sect-news" class="title-large">News</p>
<ul>
  <li><p class="content">Our vid2vid paper is accepted to NIPS 2018!</li>
  <li><p class="content">Our inpainting paper is accepted to ECCV 2018!</li>
  <li><p class="content">Received NVIDIA Pioneer Research Award from Jensen Huang.</li>
  <li><p class="content">Won 1st place in <a href="http://bdd-data.berkeley.edu/wad-2018.html" target="_blank" rel="nofollow" class="caption-2">WAD Challenge</a>, Domain Adaptation for Semantic Segmentation Competition, CVPR 2018</li>
  <li><p class="content">Presented our <a href="https://youtu.be/sIkUzmgUaxc?t=1200" target="_blank" rel="nofollow" class="caption-2">pix2pixHD</a> work at CVPR 2018.</li>
  <li><p class="content">Check out our <a href="https://twitter.com/NVIDIAAIDev/status/1009873840246280192" target="_blank" rel="nofollow" class="caption-2">face demo</a> at CVPR 2018!</li>
<br>
</ul>
-->

<p id="sect-publications" class="title-large">Publications</p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
      <tr> <td width="20%" valign="top"><img src="papers/CVPR22/OCA_BMAremoval.png" alt="OCABMA" width="100%" border="1">
        <td width="80%" valign="top">
        <p>
        <heading>Self-Supervised Bulk Motion Artifact Removal in Optical Coherence Tomography Angiography</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Jiaxiang Ren</strong>, Kicheon Park, Yingtian Pan, and Haibin Ling</p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>IEEE / CVF Computer Vision and Pattern Recognition Conference(<strong>CVPR</strong>)</em>, 2022</p>
        <p class="margin">&nbsp;</p>

        <div class="paper" id="OCABMA">
        <p class="content">
        <strong><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ren_Self-Supervised_Bulk_Motion_Artifact_Removal_in_Optical_Coherence_Tomography_Angiography_CVPR_2022_paper.pdf">Paper</a></strong> |
        <strong><a href="https://github.com/reckdk/CABR-BMARemoval">Code</a></strong> |
        <strong><a href="javascript:toggleblock('OCA_BMAremoval_abs')">Abstract</a></strong></p>
        <p align="justify"> <i id="OCA_BMAremoval_abs"> Optical coherence tomography angiography (OCTA) is an important imaging modality in many bioengineering tasks. The image quality of OCTA, however, is often hurt by Bulk Motion Artifacts (BMA), which are due to micromotion of subjects and typically appear as bright stripes surrounded by blur areas. State-of-the-art BMA handling solutions usually treat the problem as an image inpainting one with deep neural network algorithms. These solutions, however, require numerous training samples with nontrivial annotation. Nevertheless, this context-based inpainting model has limited correction capability because it discards the rich structural and appearance information carried in the BMA stripe region. To address these issues, in this paper we propose a self-supervised content-aware BMA recover model. First, the gradient-based structural information and appearance feature are extracted from the BMA area and injected into the model to capture more connectivity. Second, with easily collected defective masks, the model is trained in a self-supervised manner that only the clear areas are for training while the BMA areas for inference. With structural information and appearance feature from noisy image as references, our model could correct larger BMA and produce better visualizing result. Only 2D images with defective masks are involved so our method is more efficient. Experiments on OCTA of mouse cortex demonstrate that our model could correct most BMA with extremely large sizes and inconsistent intensities while existing methods fail. </i></p>
        <pre xml:space="preserve" style="display: none;">
      </pre> </div> </td> </tr>
      </table>
  </div>
  </div>
  </div>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
      <tr> <td width="20%" valign="top"><img src="papers/NatCommBio/enhancement.png" alt="ODTEnhance" width="100%" border="1">
        <td width="80%" valign="top">
        <p>
        <heading>Dynamic 3D imaging of cerebral blood flow in awake mice using self supervised-learning-enhanced optical coherence Doppler tomography</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content">Yingtian Pan, Kicheon Park, <strong>Jiaxiang Ren</strong>, Nora D. Volkow, Haibin Ling, Alan P. Koretsky, Congwu Du</p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>Nature Communications Biology</em>, 2023 (IF: 6.548)</p>
        <p class="margin">&nbsp;</p>

        <div class="paper" id="ODTEnhance">
        <p class="content">
        <strong><a href="https://github.com/reckdk/SSL_ODTEnhancement">Code</a></strong> |
        <strong><a href="javascript:toggleblock('ODT_Enhancement_abs')">Abstract</a></strong></p>
        <p align="justify"> <i id="ODT_Enhancement_abs"> Cerebral blood flow (CBF) is widely used to assess brain function in normal and diseased states. However, most preclinical CBF imaging is performed under anesthesia, thus brain functional changes are confounded by anesthetic effects. Due to motion artifacts and high background noise, high spatiotemporal-resolution CBF imaging of awake animals is challenging, particularly for Doppler-based flow imaging modalities. Here, we report ultrahigh-resolution optical coherence Doppler tomography (&mu;ODT) for 3D imaging of CBF velocity (CBFv) network dynamics in head- restrained awake-behaving animals by developing self-supervised deep-learning approaches for effective image denoising and motion-artifact removal. </i></p>
        <pre xml:space="preserve" style="display: none;">
      </pre> </div> </td> </tr>
      </table>
  </div>
  </div>
  </div>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
      <tr> <td width="20%" valign="top"><img src="papers/EMBC21/EMBC21.png" alt="DPR_BMD" width="100%" border="1">
        <td width="80%" valign="top">
        <p>
        <heading>Osteoporosis Prescreening and Bone Mineral Density Prediction using Dental Panoramic Radiographs</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content">Yasha Singh, Vivek Atulkar, <strong>Jiaxiang Ren</strong>, Jie Yang, Heng Fan, Longin Jan Latecki, Haibin Ling</p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (<strong>EMBC</strong>)</em>, 2021</p>
        <p class="margin">&nbsp;</p>
        <div class="paper" id="DPR_BMD">
        <p class="content">
        <strong><a href="https://reckdk.github.io/papers/EMBC21/EMBC21-final.pdf">Paper</a></strong> |
        <strong><a href="javascript:toggleblock('DPR_BMD_abs')">Abstract</a></strong></p>
        <p align="justify"> <i id="DPR_BMD_abs"> Recent studies have shown that Dental Panoramic Radiograph (DPR) images have great potential for prescreening of osteoporosis given the high degree of correlation between the bone density and trabecular bone structure. Most of the research works in these area had used pretrained models for feature extraction and classification with good success. However, when the size of the data set is limited it becomes difficult to use these pretrained networks and gain high confidence scores. In this paper, we evaluated the diagnostic performance of deep convolutional neural networks (DCNN)- based computer-assisted diagnosis (CAD) system in the detection of osteoporosis on panoramic radiographs, through a comparison with diagnoses made by oral and maxillofacial radiologists. With the available labelled dataset of 70 images, results were reproduced for the preliminary study model. Furthermore, the model performance was enhanced using different computer vision techniques. Specifically, the age meta data available for each patient was leveraged to obtain more accurate predictions. Lastly, we tried to leverage these images, ages and osteoporotic labels to create a neural network based regression model and predict the Bone Mineral Density (BMD) value for each patient. Experimental results showed that the proposed CAD system was in high accord with experienced oral and maxillofacial radiologists in detecting osteoporosis and achieved 87.86% accuracy. </i></p>
        <pre xml:space="preserve" style="display: none;">
      </pre> </div> </td> </tr>
      </table>
  </div>
  </div>
  </div>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
      <tr> <td width="20%" valign="top"><img src="papers/EMBC20/EMBC20.jpg" alt="DPR_landmark" width="100%" border="1">
        <td width="80%" valign="top">
        <p>
        <heading>Detection of Trabecular Landmarks for Osteoporosis Prescreening in Dental Panoramic Radiographs</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Jiaxiang Ren</strong>, Heng Fan, Jie Yang, and Haibin Ling</p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>42rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (<strong>EMBC</strong>)</em>, 2020</p>
        <p class="margin">&nbsp;</p>

        <div class="paper" id="DPR_landmark">
        <p class="content">
<strong><a href="https://reckdk.github.io/papers/EMBC20/EMBC20-final.pdf">Paper</a></strong> |
        <strong><a href="javascript:toggleblock('DPR_landmark_abs')">Abstract</a></strong></p>
        <p align="justify"> <i id="DPR_landmark_abs"> Dental panoramic radiography (DPR) images have recently attracted increasing attention in osteoporosis analysis because of their inner correlation. Many approaches leverage machine learning techniques (e.g. , deep convolutional neural networks (CNNs)) to study DPR images of a patient to provide initial analysis of osteoporosis, which demonstrates promising results and significantly reduces financial cost. However, these methods heavily rely on the trabecula landmarks of DPR images that requires a large amount of manual annotations by dentist, and thus are limited in practical application. Addressing this issue, we propose to automatically detect trabecular landmarks in DPR images. In specific, we first apply CNNs-based detector for trabecular landmark detection and analyze its limitations. Using CNNs-based detection as a baseline, we then introduce a statistic shape model (SSM) for trabecular landmark detection by taking advantage of spatial distribution prior of trabecular landmarks in DPR images and their structural relations. In experiment on 108 images, our solution outperforms CNNs- based detector. Moreover, compared to CNN-based detectors, our method avoids the needs of vast training samples, which is more practical in application. </i></p>
        <pre xml:space="preserve" style="display: none;">
      </pre> </div> </td> </tr>
      </table>
  </div>
  </div>
  </div>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
      <tr> <td width="20%" valign="top"><a href="https://reckdk.github.io/essc_icmew17.html" class="hoverZoomLink"><img src="papers/ICME17/illu.jpg" alt="essc_icmew" width="100%" border="1" height="20%"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Enhanced Sparse Subspace Clustering</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Jiaxiang Ren</strong>, Shengjie Zhao, Kai Yang, and Brian Zhao</p>
        <p class="margin-small">&nbsp;</p>
        <p class="content"><em>IEEE International Conference on Multimedia & Expo Workshops(<strong>ICMEW</strong>)</em>, 2017</p>
        <p class="margin">&nbsp;</p>

        <div class="paper" id="essc_icmew">
        <p class="content">
        <strong><a href="./essc_icmew17.html">Project</a></strong> |
        <strong><a href="https://reckdk.github.io/papers/ICME17/ESSC-ICMEW17.pdf">Paper</a></strong> |
        <strong><a href="https://reckdk.github.io/papers/ICME17/Report_ESSC.pdf">Slides</a></strong> |
				<strong><a shape="rect" href="javascript:togglebib('essc_icmew')" class="togglebib">Bibtex</a></strong> |
        <strong><a href="javascript:toggleblock('essc_icmew_abs')">Abstract</a></strong>         </p>
        <p align="justify"> <i id="essc_icmew_abs"> High-dimensional data are ubiquitous in most real-world research areas, such as machine learning, image processing and so on. Actually, high-dimensional data that belong to the same classes tend to gather in their own low-dimensional subspaces. Recently, many subspace recovery algorithms based on the sparse representation such as Sparse Subspace Clustering (SSC), Low-Rank Representation (LRR) and their variants are proposed to address the subspace clustering problems.<br />
        In this paper, we propose a novel clustering method based on SSC, called Enhanced Sparse Subspace Clustering (ESSC), to deal with complicated face images under variant expressions, illuminations or disguises. Assuming that the variant expressions or disguises of the face images are sharable, we introduce the adaptive difference dictionary to extend the simple linear combination of face images in SSC with the combination of the specific features and the common features. Thus both of the accuracy and generalization of clustering are improved simultaneously.The experimental results on the AR face databases show that the proposed ESSC not only makes up to 9.0% improvements on accuracy compared with SSC, but also is more robust and scalable for dealing with larger face clustering problems (up to 400 samples from 100 subjects) under variant expressions, disguises or illuminations. </i></p>
        <pre xml:space="preserve" style="display: none;">
@inproceedings{Ren2017essc,
  title={A novel and robust face clustering method via adaptive difference dictionary},
  author={Jiaxiang Ren and Shengjie Zhao and Kai Yang and Brian Nlong Zhao},
  booktitle={IEEE International Conference on Multimedia & Expo Workshops (ICMEW)},
  pages={627-632},
  year={2017},
}
      </pre> </div> </td> </tr>
      </table>
  </div>
  </div>
  </div>
<div id="footer">
</div>
<br>
<br>
<p id="sect-projects" class="title-large">Projects</p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
      <tr> <td width="20%" valign="top"><img src="projects/TB/TB_illu.jpg" alt="TB" width="100%" border="1">
        <td width="80%" valign="top">
        <p>
        <heading>Tuberculosis Screening</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Jiaxiang Ren</strong>, Lingyun Huang</p>
        <p class="margin-small">&nbsp;</p>
        <p class="content">Project
        <p class="margin">&nbsp;</p>

        <div class="project" id="TB">
        <p class="content">
        <strong><a href="javascript:toggleblock('TB_abs')">Abstract</a></strong></p>
        <p align="justify"> <i id="TB_abs"> Tuberculosis is a serious disease that affects lots of people in developing countries. To treat TB patients,  an accurate diagnosis is needed firstly.
					X-ray radiography is often used for TB screening among high risk populations but manual examining cheXray is time consuming. For improving efficiency and reducing screening cost, we develop a series models to classify normal and TB patients' cheXray images, find and mark related infections within chest. <br \>
				According to the distribution of our TB dataset, we experiment with different segmentation and detection models, such as attention-based DANet and anchor-free FCOS. Our final model is specifically designed for small infections, which improve recall with insignificant loss in precision.
				</i></p>

        <pre xml:space="preserve" style="display: none;">
      </pre> </div> </td> </tr>

      </table>
  </div>
  </div>
  </div>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
      <tr> <td width="20%" valign="top"><a href="./RSNA_Pneumonia.html" class="hoverZoomLink"><img src="projects/RSNA_Pneumonia/illu.jpg" alt="RSNA_Pneu" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>RSNA Pneumonia Detection Challenge (Kaggle)</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Jiaxiang Ren</strong>, Liangxin Gao, Yanbo Zhang</p>
        <p class="margin-small">&nbsp;</p>
        <p class="content">Kaggle Competition (Rank: 39/1499, Top 3%)
        <p class="margin">&nbsp;</p>

        <div class="project" id="RSNA_Pneu">
        <p class="content">
        <strong><a href="./RSNA_Pneumonia.html">Project</a></strong> |
        <strong><a href="javascript:toggleblock('RSNA_Pneu_abs')">Abstract</a></strong></p>
        <p align="justify"> <i id="RSNA_Pneu_abs"> Pneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. To improve the efficiency and reach of diagnostic services, the <a href="https://www.rsna.org">Radiological Society of North America (RSNA)</a> has reached out to machine learning community of <a href="https://www.kaggle.com">Kaggle</a> and collaborated with the <a href="https://www.nih.gov">US National Institutes of Health</a>, The <a href="https://thoracicrad.org">Society of Thoracic Radiology</a>, and <a href="https://www.md.ai">MD.ai</a> to develop a rich dataset for this challenge. <br\>
				Our final model is mainly based on <a href="https://github.com/qqwweee/keras-yolo3">keras-YOLOv3</a> with Hard Negative Mining(HNM) to reduce false positive rates. Among 1499 competitors, our team finally achieved the 39th place (Top 3%).
				</i></p>

        <pre xml:space="preserve" style="display: none;">
      </pre> </div> </td> </tr>

      </table>
  </div>
  </div>
  </div>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
	      <tr> <td width="20%" valign="top"><img src="projects/herb/herb_illu.jpg" alt="herb" width="100%" border="1">
	        <td width="80%" valign="top">
	        <p>
	        <heading>Fine-grained Plants Categorization</heading></a><br>
	        <p class="margin">&nbsp;</p>
	        <p class="content"><strong>Jiaxiang Ren</strong>, Aimee Liu, Jin Ma</p>
	        <p class="margin-small">&nbsp;</p>
	        <p class="content">Project
	        <p class="margin">&nbsp;</p>

	        <div class="project" id="herb">
	        <p class="content">
	        <strong><a href="javascript:toggleblock('herb_abs')">Abstract</a></strong></p>
	        <p align="justify"> <i id="herb_abs"> Atractylodes lancea is a high-demand Chinese herbal medicine (annual output > 1,000 tons). The main medicinal part of Atractylodes lancea is the root, which is very similar to the root of Atractylodes macrocephala.
						Only experienced personnel could distinguish them correctly. As a result, it is difficult to meet the requirement of production and purity at the same time. Separating these two similar plants during cultivating is a solution to increase the yield of high-purity medicine.
						However, when cultivating, most plants are without distinguishable flowers as illustrated in the left image. To this end, we take advantage of transfer-learning to solve this fine-grained categorization problem. <br \>
					After analysing all the samples in our herb dataset, we process image into affordable size while its discriminative features are preserved. Thereafter, we use transfer-learning and other regular tricks to train our model.<br \>
					*This illustration image is from <a href="https://library.hkbu.edu.hk/electronic/libdbs/mpd/">Medicinal Plant Images Database</a>.
					</i></p>

	        <pre xml:space="preserve" style="display: none;">
	      </pre> </div> </td> </tr>

	      </table>
	  </div>
	  </div>
	  </div>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
      <tr> <td width="20%" valign="top"><a href="./3d_recon.html" class="hoverZoomLink"><img src="projects/3d_recon/illu_improved_ssc.jpg" alt="3d_recon" width="100%" border="1"></a>
        <td width="80%" valign="top">
        <p>
        <heading>Improvement of 3D Semantic Completion from a Single Depth Image</heading></a><br>
        <p class="margin">&nbsp;</p>
        <p class="content"><strong>Jiaxiang Ren</strong>, Shengjie Zhao</p>
        <p class="margin-small">&nbsp;</p>
        <p class="content">Graduate Research
        <p class="margin">&nbsp;</p>

        <div class="project" id="3d_recon">
        <p class="content">
        <strong><a href="./3d_recon.html">Project</a></strong> |
        <strong><a href="javascript:toggleblock('3d_recon_abs')">Abstract</a></strong></p>
        <p align="justify"> <i id="3d_recon_abs"> Project aims at completing the 3D semantic scene from the depth images.
We introduced a post-processing module to improve the Average Precision (AP) and Intersection-over-union (IoU). The aggregated end-to-end network improves the AP by 6.1% and IoU by 2.3%. </i></p>
        <pre xml:space="preserve" style="display: none;">
      </pre> </div> </td> </tr>

      </table>
  </div>
  </div>
  </div>

	<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
	      <tr> <td width="20%" valign="top"><a href="./thesis_master.html" class="hoverZoomLink"><img src="projects/thesis_master/thumbnail.jpg" alt="thesis_master" width="100%" height="20%" border="1"></a>
	        <td width="80%" valign="top">
	        <p>
	        <heading>The Research on the Sparse-based Subspace Clustering Algorithm in High-dimensional Data</heading></a><br>
	        <p class="margin">&nbsp;</p>
	        <p class="content"><strong>Jiaxiang Ren</strong>, Shengjie Zhao</p>
	        <p class="margin-small">&nbsp;</p>
	        <p class="content">Master Thesis
	        <p class="margin">&nbsp;</p>

	        <div class="project" id="thesis_master">
	        <p class="content">
	        <strong><a href="./thesis_master.html">Project</a></strong> |
	        <strong><a href="javascript:toggleblock('thesis_master_abs')">Abstract</a></strong></p>
	        <p align="justify"> <i id="thesis_master_abs"> High-dimensional data is ubiquitous in machine learning, pattern recognition, image processing, computer vision, and bioinformatics. It causes “Curse of dimensionality”, which not only increases the cost but also introduces noise data. However, high-dimensional data is usually gathered in the low-dimensional subspaces. This property brings hope for us to solve the “dimensional curse” problem. This paper research on the sparse subspace clustering algorithm. The effectiveness of the proposed algorithm for complex noises is verified through experiments.<br \>
					By constructing an adaptive difference dictionary, the enhanced algorithm can better restore the structure of the underlying subspace and improve the accuracy. The randomized algorithm is optional to the adaptive difference dictionary, which can greatly increase the speed of calculation when degrades little performance. So it is suitable for environments with limited computing power.</i></p>
	        <pre xml:space="preserve" style="display: none;">
	      </pre> </div> </td> </tr>

	      </table>
	  </div>
	  </div>
	  </div>
<div id="footer">
</div>

<br>
<br>
<p id="sect-honors" class="title-large">Awards</p>
<ul>
<li>Excellent Graduate of Shanghai, China, 2018</li>
<li>The ENN Energy Scholarship, China, 2017</li>
<li>The 13th National Postgraduate Mathematic Contest in Modeling, third-prize, China, 2016</li>
<li>The 29th National College Physics Competition (Shanghai), third-prize, China, 2012</li>
<li>Provincial Outstanding Students, China (Top 0.02%), 2011</li>
</ul>
</div>
</div>
</div>
<div id="footer">
</div>


<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('OCA_BMAremoval_abs');
hideblock('ODT_Enhancement_abs');
hideblock('DPR_BMD_abs');
hideblock('DPR_landmark_abs');
hideblock('essc_icmew_abs');
hideblock('herb_abs');
hideblock('TB_abs');
hideblock('RSNA_Pneu_abs');
hideblock('3d_recon_abs');
hideblock('thesis_master_abs');
</script>


<div style="display:none">
<!-- GoStats JavaScript Based Code -->
<script type="text/javascript" src="./files/counter.js.download"></script><script language="javascript">var _go_js="1.0";</script><script language="javascript1.1">_go_js="1.1";</script><script language="javascript1.2">_go_js="1.2";</script><script language="javascript1.3">_go_js="1.3";</script><script language="javascript1.4">_go_js="1.4";</script><script language="javascript1.5">_go_js="1.5";</script><script language="javascript1.6">_go_js="1.6";</script><script language="javascript1.7">_go_js="1.7";</script><script language="javascript1.8">_go_js="1.8";</script><script language="javascript1.9">_go_js="1.9";</script><script language="javascript"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script><a target="_blank" href="http://gostats.com/" title="web page statistics from GoStats"><img id="_go_render_39058369" alt="web page statistics from GoStats" title="web page statistics from GoStats" border="0" style="border-width:0px" src="./files/count"></a>
<!-- <a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"> -->
</div>
<!--
<img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" />
</a>
-->
<!-- End GoStats JavaScript Based Code -->

</body></html>
